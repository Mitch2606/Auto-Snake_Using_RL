numObsSnake = 17;
numObsReward = numObsSnake + 1;

numActSnake = 3;
numActReward = 11;

numTimeSteps = 5

%Snake Model
ObservationInfo = rlNumericSpec([1 numObsSnake]);
ObservationInfo.Name = 'Agent Actions';

ActionInfo = rlFiniteSetSpec([1 2 3]);
ActionInfo.Name = 'Direction';

actorNet = makeActorNet(numObsSnake, numActSnake, ObservationInfo.Name);
criticNet = makeCriticNet(numObsSnake, ObservationInfo.Name);

repOpts = rlRepresentationOptions('LearnRate',5e-2,'GradientThreshold',1);

discActor = rlStochasticActorRepresentation(actorNet,ObservationInfo,ActionInfo,'Observation',ObservationInfo.Name);
critic = rlValueRepresentation(criticNet,ObservationInfo,'Observation',ObservationInfo.Name,repOpts);

Paul = rlACAgent(discActor, critic);

%Reward Model
ObservationInfoReward = rlNumericSpec([1 numObsReward]);
ObservationInfoReward.Name = 'Agent Actions';

ActionInfo = rlFiniteSetSpec([-5 -4 -3 -2 -1 0 1 2 3 4 5]);
ActionInfo.Name = 'Agent Reward';

actorNet = makeActorNet(numObsReward, numActReward, ObservationInfoReward.Name);
criticNet = makeCriticNet(numObsReward, ObservationInfoReward.Name);

repOpts = rlRepresentationOptions('LearnRate',5e-2,'GradientThreshold',1);

discActor = rlStochasticActorRepresentation(actorNet,ObservationInfoReward,ActionInfo,'Observation',ObservationInfoReward.Name);
critic = rlValueRepresentation(criticNet,ObservationInfoReward,'Observation',ObservationInfoReward.Name, repOpts);

Tony = rlPPOAgent(discActor, critic);

%Initialize Environment
env = rlSimulinkEnv("rlPlaysSnakeV3", ["rlPlaysSnakeV3/Paul", "rlPlaysSnakeV3/Tony"]);

